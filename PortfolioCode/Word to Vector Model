{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZZcwew9OLhiF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/sample_data/River-and-Rail-Cantina.csv\")"
      ],
      "metadata": {
        "id": "dlEZy4sHL3pJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['user_name'] = df['user_name'].astype(str)\n",
        "df['rating'] = df['rating'].astype(str)\n",
        "df['rating'] = df['rating'].str.replace(' star rating', '')\n",
        "df['review_text'] = df['review_text'].astype(str)"
      ],
      "metadata": {
        "id": "r1NkbNVzL5W4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['user_name'] = df['user_name'].str.strip(\"['']\")\n",
        "df['rating'] = df['rating'].str.strip(\"['']\")\n",
        "df['review_text'] = df['review_text'].str.strip(\"['']\")"
      ],
      "metadata": {
        "id": "CBUlisyuL634"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['rating'] = df['rating'].str.split(\"'\", expand=True).get(0) #Some people reviewed the restaurant twice and I couldn't figure out how to split the reviews so I just deleted the second review\n",
        "df['user_name'] = df['user_name'].str.split(\"'\", expand=True).get(0) #Some people had the amount of photos posted so I cleaned it up\n",
        "df['review_text'] = df['review_text'].str.split(\", '\", expand=True).get(0) #same as the ratings problem"
      ],
      "metadata": {
        "id": "VxM_U3EUL8ZI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['rating'] = df['rating'].astype(int)\n",
        "df.drop(\"Unnamed: 0\", axis=1, inplace=True) #I do not know where unnamed:  0 came from so I dropped it"
      ],
      "metadata": {
        "id": "W6blF9TaL-zO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recode_rating(rating):\n",
        "    if rating <= 3:\n",
        "        return 'Negative'\n",
        "    elif rating <= 5:\n",
        "        return 'Positive'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "df['sentiment'] = df['rating'].apply(recode_rating)"
      ],
      "metadata": {
        "id": "jdXVQRzuPprf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pyWCcHWPqyb",
        "outputId": "0eab8694-4b44-4a67-d020-e8e4dbc726e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "sbwNu6PFPtUK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_review(review_text):\n",
        "    # Remove punctuation\n",
        "    review_text = review_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # Convert all words to lower case\n",
        "    review_text = review_text.lower()\n",
        "    \n",
        "    # Tokenize the review into words\n",
        "    words = word_tokenize(review_text)\n",
        "    \n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    \n",
        "    # Join the remaining words back into a single string\n",
        "    processed_review = ' '.join(words)\n",
        "    \n",
        "    return processed_review\n",
        "\n",
        "df['review_text'] = df['review_text'].apply(preprocess_review)"
      ],
      "metadata": {
        "id": "r2_4YuR_PwcA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "D4kpqNWJQHFH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_reviews = []\n",
        "for review in df['review_text']:\n",
        "    tokens = tokenizer.tokenize(review.lower())\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    normalized_reviews.append(filtered_tokens)"
      ],
      "metadata": {
        "id": "9QB1Av6xQKCT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v_model = Word2Vec(normalized_reviews, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "X5m-Jz6GOW2w"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=['quesadillas'], topn=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgguGjexQbih",
        "outputId": "63eb06a9-4fe0-4f7c-c30c-3d29bf3821c4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hard', 0.9974542856216431),\n",
              " ('mexican', 0.9973438382148743),\n",
              " ('sit', 0.9973161220550537)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first two make sense but I do not really understand sit."
      ],
      "metadata": {
        "id": "0Xhoia6_RFKc"
      }
    }
  ]
}
